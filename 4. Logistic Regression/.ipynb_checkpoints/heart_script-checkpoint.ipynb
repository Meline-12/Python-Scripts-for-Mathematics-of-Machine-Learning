{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d27854b",
   "metadata": {},
   "source": [
    "# Mathematics of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb41502",
   "metadata": {},
   "source": [
    "## Programming tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa65dd",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7d1a6",
   "metadata": {},
   "source": [
    "### a) Preparation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data set\n",
    "T = np.loadtxt('heart.dat')\n",
    "# print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the real features\n",
    "X = T[:, [0, 3, 4, 7, 9, 11]]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and transform the markers\n",
    "Y = 2 * T[:, 13] - 3\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f043d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data pairs\n",
    "m = len(Y)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "d = np.size(X, axis=1)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4445a5c",
   "metadata": {},
   "source": [
    "### b) Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec767c52",
   "metadata": {},
   "source": [
    "Random selection of the indices of the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b632d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of training data\n",
    "p = 0.7\n",
    "data_ind = np.random.permutation(m)\n",
    "print((np.ceil(p*m)+1))\n",
    "ind_train = data_ind[:int((np.ceil(p*m)+1))]\n",
    "ind_test = [i for i in data_ind if i not in ind_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = X[ind_train, :]\n",
    "Y_train = Y[ind_train]\n",
    "# print(X_train)\n",
    "# print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "X_test = X[ind_test, :]\n",
    "Y_test = Y[ind_test]\n",
    "# print(X_test)\n",
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8bd04e",
   "metadata": {},
   "source": [
    "### c) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb73552",
   "metadata": {},
   "source": [
    "NOTE: We include the bias in the last position in the vector w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical Risk Function\n",
    "# RS_log = @(w) mean( log(1 + exp(- Y_train .* (X_train * w(1:d) + w(end)))) , 1)\n",
    "def RS_log(w): return np.mean(np.log(1 + np.exp(-(np.multiply(Y_train, np.dot(X_train, w[0:d]) + w[-1])))), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67028ba",
   "metadata": {},
   "source": [
    "Numerical calculation of ERM parameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6714b",
   "metadata": {},
   "source": [
    "... for this we allow enough iteration and choosing a random starting value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a69fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.normal(size=(d+1,1))\n",
    "w_LR, RS_min, iter, funcalls, warnflag = opt.fmin(RS_log, np.zeros((7, 1)), maxfun=100000, full_output=True)\n",
    "\n",
    "print(w_LR)\n",
    "print(RS_min)\n",
    "print(iter)\n",
    "print(funcalls)\n",
    "print(warnflag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the misclassified training data via constraint violation:\n",
    "Err_Train = np.mean(np.multiply(Y_train, np.dot(X_train, w_LR[0:d]) + w_LR[-1]) < 0)\n",
    "print(\"{:.1f} percent of the training data is misclassified.\".format(Err_Train * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af683a9",
   "metadata": {},
   "source": [
    "ANSWER: If the sample were linearly separable, logistic regression would find the appropriate separating hypothesis. Because of the existing misclassifications, this is not that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a328f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the misclassified test data via constraint violation:\n",
    "Err_Test = np.mean(np.multiply(Y_test, np.dot(X_test, w_LR[0:d]) + w_LR[-1]) < 0)\n",
    "print(\"{:.1f} percent of the test data is misclassified.\".format(Err_Test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7943b",
   "metadata": {},
   "source": [
    "#### ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"So we estimate the expected risk of h_S to be {:.1f} percent.\".format(Err_Test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130ec0e",
   "metadata": {},
   "source": [
    "### d) Soft-margin SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa34a56",
   "metadata": {},
   "source": [
    "Choice of lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa41d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both terms equally weighted\n",
    "lam = 1/m\n",
    "Y_train = Y_train[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def hinge(w, x, y): \n",
    "    # return np.amax(np.append(1 - np.multiply(y, (np.dot(x, w[0:d]) + w[-1])), np.zeros((len(y), 1)), axis = 1), axis = 1)\n",
    "    return np.amax(np.append(1 - np.multiply(Y_train, (np.dot(X_train, w[0:d]) + w[-1])), np.zeros((len(Y_train), 1)), axis = 1), axis = 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the solution\n",
    "def fun(w): return lam * np.linalg.norm(w[0:d])**2 + np.mean(hinge(w, X_train, Y_train))\n",
    "\n",
    "# w_SVM, RS_min, iter, funcalls, warnflag = opt.fmin(fun, np.random.randn(d+1, 1), maxfun=100000, full_output=True)\n",
    "result = opt.minimize(fun, np.random.randn(d+1, 1), options={'disp': True})\n",
    "\n",
    "w_SVM = result.x\n",
    "RS_min = result.fun\n",
    "\n",
    "print(w_SVM)\n",
    "print(RS_min)\n",
    "# print(iter)\n",
    "# print(funcalls)\n",
    "# print(warnflag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the misclassified training data via constraint violation:\n",
    "Err_Train = np.mean(np.multiply(Y_train, np.dot(X_train, w_SVM[0:d]) + w_SVM[-1]) < 0)\n",
    "print(\"{:.1f} percent of the training data is misclassified.\".format(Err_Train * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the misclassified test data via constraint violation:\n",
    "Err_Test = np.mean(np.multiply(Y_test, np.dot(X_test, w_SVM[0:d]) + w_SVM[-1]) < 0)\n",
    "print(\"{:.1f} percent of the test data is misclassified.\".format(Err_Test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"So we estimate the expected risk of h_S to be {:.1f} percent.\".format(Err_Test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ab3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
