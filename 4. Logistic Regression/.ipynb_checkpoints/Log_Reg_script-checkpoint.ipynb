{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7ee946",
   "metadata": {},
   "source": [
    "# Mathematics of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e989e1f",
   "metadata": {},
   "source": [
    "## Programming tasks: Examples for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725eed2",
   "metadata": {},
   "source": [
    "### (0) Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dad73",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49825c0",
   "metadata": {},
   "source": [
    "Generate the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataset\n",
    "m = 50\n",
    "x = np.random.uniform(low=-3, high=3, size=(2, m))\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f478a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true parameters\n",
    "w_true = np.array([[1], [2]])\n",
    "print(w_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities for label +1 according to the Bernoulli model with h_{w_true,0}\n",
    "p = 1/(1 + np.exp(-(np.dot(w_true.T, x))))\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e962562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice out the random markers according to the probabilities p\n",
    "y = 2*(np.random.uniform(low=0, high=1, size=(1, m)) <= p) - 1\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bafc4",
   "metadata": {},
   "source": [
    "### (1) Graphical visualization of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e867595",
   "metadata": {},
   "source": [
    "First plot the true hyperplane for x in [-3,3]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211a400",
   "metadata": {},
   "source": [
    "Then enter the classified points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points with mark 1\n",
    "inds = [i for (i, val) in enumerate(y[0]) if val == 1]\n",
    "print(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465543d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points with mark -1\n",
    "indm = [i for (i, val) in enumerate(y[0]) if val == -1]\n",
    "print(indm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc65a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot the true hyperplane for x in [-3,3].\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([-3,3], -w_true[0]/w_true[1]*[-3,3], \"--\", label=\"true hyperplane\")\n",
    "ax.scatter(x[0][inds], x[1][inds], c=\"b\", marker=\"+\", linewidths = 2)\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"d\", linewidths = 2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "ax.axis('equal')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6e36b",
   "metadata": {},
   "source": [
    "### (2) Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6d09e",
   "metadata": {},
   "source": [
    "Empirical risk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RS(w): return np.mean(np.log(1 + np.exp(- np.multiply(y, (np.dot(w.T, x))))), axis=1)\n",
    "# RS = @(w) mean( log(1 + exp(- y .* (w' * x))),2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87714d",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "<br> 1) w is a column vector with two rows OR a matrix with two rows and multiple columns for multiple weight vectors w.\n",
    "<br> 2) mean(. ,2) calculates the mean value per column\n",
    "<br> 3) The function should be evaluable for multiple w vectors, so that the result contains as row vector the empirical risk values of the individual w vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063131d1",
   "metadata": {},
   "source": [
    "Plot this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90473e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization of the w-values per axis\n",
    "ws = np.array([[i] for i in np.arange(-5, 10 + 0.01, 0.01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18fa196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discretization grid\n",
    "WW1, WW2 = np.meshgrid(ws, ws)\n",
    "WW1_ravel = np.ravel(WW1, order='F')\n",
    "WW2_ravel = np.ravel(WW2, order='F')\n",
    "print(WW1_ravel)\n",
    "print(WW2_ravel)\n",
    "print(len(WW2_ravel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47697b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RS at the grid points\n",
    "RS_Ws = RS(np.concatenate((WW1_ravel[:, None], WW2_ravel[:, None]), axis=1).T)\n",
    "print(np.concatenate((WW1_ravel[:, None], WW2_ravel[:, None]), axis=1).T)\n",
    "print(RS_Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graphic (contour plot):\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "CS = ax.contour(WW1, WW2, np.reshape(np.log(RS_Ws), (len(ws), len(ws))), 25)\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "\n",
    "ax.set_title('log R_S(h_w)')\n",
    "ax.set_xlabel('w_1')\n",
    "ax.set_ylabel('w_2')\n",
    "\n",
    "fig.colorbar(CS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aba9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical calculation of the ERM parameters\n",
    "# result = opt.fmin_tnc(RS,  np.zeros((2, 1)))\n",
    "# xopt, fopt, iter, funcalls, warnflag = fmin(T,0, full_output=True, disp=False)\n",
    "w, RS_min, iter, funcalls, warnflag = opt.fmin(RS,  np.zeros((2, 1)), maxiter=1000, full_output=True, disp=False)\n",
    "\n",
    "print(w)\n",
    "print(RS_min)\n",
    "print(iter)\n",
    "print(funcalls)\n",
    "print(warnflag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc13742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learned parameters\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "CS = ax.contour(WW1, WW2, np.reshape(np.log(RS_Ws), (len(ws), len(ws))), 10)\n",
    "ax.clabel(CS, inline=True, fontsize=10)\n",
    "\n",
    "# learned\n",
    "ax.scatter(w[0], w[1], c=\"r\")\n",
    "# true\n",
    "# ax.scatter(w_true[0], w_true[1], marker=\"+\")\n",
    "\n",
    "ax.set_title('log R_S(h_w)')\n",
    "ax.set_xlabel('w_1')\n",
    "ax.set_ylabel('w_2')\n",
    "\n",
    "fig.colorbar(CS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e0e98",
   "metadata": {},
   "source": [
    "### Hypothesen und Daten zeichnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot([-3,3], -w_true[0]/w_true[1]*[-3,3], \"--\", label=\"True hypothesis\")\n",
    "ax.plot([-3,3], -w[0]/w[1]*np.array([-3,3]), c=\"g\", label=\"learned hypothesis\")\n",
    "ax.scatter(x[0][inds], x[1][inds], c=\"b\", marker=\"+\", linewidths = 2)\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"d\", linewidths = 2)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "ax.axis('equal')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258c54b",
   "metadata": {},
   "source": [
    "### (3) Logistic regression in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "     def forward(self, x):\n",
    "         outputs = torch.sigmoid(self.linear(x))\n",
    "         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x.T.astype(np.float32))\n",
    "print(x.size())\n",
    "# y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "for ind, val in enumerate(y[0]):\n",
    "    if val == 1:\n",
    "        y[0][ind] = +1\n",
    "    else:\n",
    "        y[0][ind] = 0\n",
    "\n",
    "y = torch.from_numpy(y.astype(np.float32))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning some hyper-parameters:\n",
    "epochs = 100 # Indicates the number of passes through the entire training dataset the network has completed\n",
    "input_dim = 2\n",
    "output_dim = 1 # Single output \n",
    "# learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805cc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss\n",
    "criterion = torch.nn.BCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBFGS: Implements L-BFGS algorithm, heavily inspired by minFunc\n",
    "optimizer = torch.optim.LBFGS(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "        outputs = model(x)\n",
    "        loss = criterion(torch.squeeze(outputs), y[0]) \n",
    "        loss.backward() \n",
    "        # print(list(model.parameters()))\n",
    "        # print(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure) # Updates weights and biases with the optimizer\n",
    "# print(loss.item())\n",
    "# print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "    outputs = model(x)\n",
    "    loss = criterion(torch.squeeze(outputs), y[0]) \n",
    "    loss.backward() \n",
    "    optimizer.step() # Updates weights and biases with the optimizer\n",
    "    \n",
    "print(loss.item())\n",
    "print(list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
