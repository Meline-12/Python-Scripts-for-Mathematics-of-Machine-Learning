{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e81e80e",
   "metadata": {},
   "source": [
    "## Mathematics of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68ef9b",
   "metadata": {},
   "source": [
    "### Chapter 3: Linear classification methods\n",
    "### Section 3.4: Soft SVM Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d92ca8",
   "metadata": {},
   "source": [
    "#### (0) Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499f6bd",
   "metadata": {},
   "source": [
    "Generate the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89972a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataset\n",
    "m = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "x = np.random.uniform(low=-3, high=3, size=(2, m))\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true parameters\n",
    "w_true = np.array([[1], [2]])\n",
    "# print(w_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities for label +1 according to the Bernoulli model with h_{w_true,0}\n",
    "p = 1/(1 + np.exp(-(np.dot(w_true.T, x))))\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice out the random markers according to the probabilities p\n",
    "y = 2*(np.random.uniform(low=0, high=1, size=(1, m)) <= p) - 1\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d31740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x = np.genfromtxt(\"data_svm_soft_X.csv\", delimiter=',')\n",
    "y = np.genfromtxt(\"data_svm_soft_Y.csv\", delimiter=',')\n",
    "y = np.array([y])\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d7614",
   "metadata": {},
   "source": [
    "#### (1) Soft SVM Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both terms are equally weighted\n",
    "lam = 1/m\n",
    "print(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def hinge(w, x, y): \n",
    "    return np.amax(np.append(1 - np.multiply(y, np.dot(w.T, x)), np.zeros((1, m)), axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the solution\n",
    "def fun(w): return lam * np.linalg.norm(w)**2 + np.mean(hinge(w, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_S, RS_min, iter, funcalls, warnflag = opt.fmin(fun, np.zeros((2, 1)), maxfun=100000, full_output=True)\n",
    "print(w_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_S = np.array([[i] for i in w_S])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f93e8",
   "metadata": {},
   "source": [
    " #### (1.1) Plot the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize w1 and w2\n",
    "w1 = np.array([[i] for i in np.arange(-10*abs(w_S[0]), 10*abs(w_S[0]) + 20*abs(w_S[0])/1000, 20*abs(w_S[0])/1000)]) \n",
    "w2 = np.array([[i] for i in np.arange(-10*abs(w_S[1]), 10*abs(w_S[1]) + 20*abs(w_S[1])/1000, 20*abs(w_S[1])/1000)]) \n",
    "print(w1.shape)\n",
    "print(w2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate discretization grid\n",
    "WW1, WW2 = np.meshgrid(w1, w2)\n",
    "W1 = np.ravel(WW1, order='F')\n",
    "W2 = np.ravel(WW2, order='F')\n",
    "FW = np.zeros((len(W1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf64f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(W1)):\n",
    "    ww = np.array([[W1[i]], [W2[i]]])\n",
    "    FW[i] = fun(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graphic (contour plot)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "CS = ax.contour(WW1, WW2, np.reshape(np.log(FW), (len(w1), len(w2))), 25)\n",
    "# ax.clabel(CS, inline=True, fontsize=10)\n",
    "\n",
    "# plot learned value\n",
    "ax.scatter(w_S[0], w_S[1], c=\"r\")\n",
    "# true\n",
    "# ax.scatter(w_true[0], w_true[1], marker=\"+\")\n",
    "\n",
    "ax.set_title('log({:.2f} |w|^2 + R_S(w))'.format(lam))\n",
    "ax.set_xlabel('w_1')\n",
    "ax.set_ylabel('w_2')\n",
    "\n",
    "fig.colorbar(CS)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176340e0",
   "metadata": {},
   "source": [
    "#### (2) Plot the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# First plot the true hyperplane for x in [-3,3]\n",
    "ax.plot([-3,3], -w_true[0]/w_true[1]*[-3,3], \"--k\", label=\"true hyperplane\")\n",
    "\n",
    "# Plot the learned hypothesis\n",
    "ax.plot([-3,3], -w_S[0]/w_S[1]*[-3,3], \"--\", c=\"g\", label=\"SVM learned hyperplane\")\n",
    "\n",
    "# For comparison: plot logistic regression\n",
    "def RS_log(w): return np.mean(np.log(1 + np.exp(- np.multiply(y, (np.dot(w.T, x))))), axis=1)\n",
    "w_LR, RS_min, iter, funcalls, warnflag = opt.fmin(RS_log, np.zeros((2, 1)), maxfun=100000, full_output=True)\n",
    "w_LR = np.array([[i] for i in w_LR])\n",
    "ax.plot([-3,3], -w_LR[0]/w_LR[1]*[-3,3], \"--\", c=\"m\", label=\"Log-Reg learned hyperplane\")\n",
    "\n",
    "# Then enter the classified points\n",
    "inds = [i for (i, val) in enumerate(y[0]) if val == 1]\n",
    "indm = [i for (i, val) in enumerate(y[0]) if val == -1]\n",
    "\n",
    "ax.scatter(x[0][inds], x[1][inds], c=\"b\", marker=\"+\", linewidths = 2)\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"d\", linewidths = 2)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"x_1\")\n",
    "plt.ylabel(\"x_2\")\n",
    "\n",
    "ax.set(xlim=(-3, 3), ylim=(-3, 3))\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
