{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b060ed",
   "metadata": {},
   "source": [
    "## Mathematics of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38648304",
   "metadata": {},
   "source": [
    "### Chapter 3: Linear classification methods\n",
    "### Section 3.5: Kernel SVM Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a05b1f",
   "metadata": {},
   "source": [
    "#### (0) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x = np.genfromtxt(\"data_KSVM_X.csv\", delimiter=',')\n",
    "y = np.genfromtxt(\"data_KSVM_Y.csv\", delimiter=',')\n",
    "\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# print(y)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "indp = [i for (i, val) in enumerate(y) if val == 1]\n",
    "indm = [i for (i, val) in enumerate(y) if val == -1]\n",
    "\n",
    "ax.scatter(x[0][indp], x[1][indp], c=\"b\", marker=\"o\", linewidths = 2)\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"+\", linewidths = 2)\n",
    "\n",
    "plt.xlabel(\"x_1\")\n",
    "plt.ylabel(\"x_2\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3598f6",
   "metadata": {},
   "source": [
    "#### (1) Soft Kernel SVM rule (Gaussian kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda for Soft Rule\n",
    "m = len(y)\n",
    "lam = 0.5/m\n",
    "\n",
    "print(m)\n",
    "print(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528983b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling parameter\n",
    "kappa = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ec1f9",
   "metadata": {},
   "source": [
    "#### gamma == kappa ####\n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html \n",
    "1.4.6 Kernel functions\n",
    "\n",
    "When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html \n",
    "1.4.7 Mathematical formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606337aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a support vector classifier\n",
    "KSVM = svm.SVC(C=0.5/m/lam, kernel='rbf', gamma=kappa)\n",
    "\n",
    "# Learn on the train subset\n",
    "KSVM.fit(x.T, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef861b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get support vectors\n",
    "support_vectors = KSVM.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcf231",
   "metadata": {},
   "source": [
    "#### (2) Plot the learned hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate discretization grid\n",
    "x_min = -1.75\n",
    "x_max = 2.25\n",
    "y_min = -1.75\n",
    "y_max = 2.25\n",
    "\n",
    "XX1, XX2 = np.mgrid[x_min:x_max + 4/500:4/500, y_min:y_max + 4/500:4/500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision function\n",
    "Z = KSVM.decision_function(np.c_[XX1.ravel(), XX2.ravel()])\n",
    "print('Z', Z)\n",
    "\n",
    "Z = Z.reshape(XX1.shape)\n",
    "# print(Z)\n",
    "# print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb68c5",
   "metadata": {},
   "source": [
    "#### (2.1) Plot the dividing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "indp = [i for (i, val) in enumerate(y) if val == 1]\n",
    "indm = [i for (i, val) in enumerate(y) if val == -1]\n",
    "\n",
    "ax.scatter(x[0][indp], x[1][indp], c=\"b\", marker=\"o\", linewidths = 2, label = \"1\")\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"+\", linewidths = 2, label = \"-1\")\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(support_vectors[:, 0], support_vectors[:, 1], marker=\"o\", markersize=12, \n",
    "         markerfacecolor=\"None\", linestyle='None', label = \"Support Vectors\")\n",
    "\n",
    "# Plot learned dividing lines\n",
    "plt.contour(XX1, XX2, Z, levels = 0)  # label = \"$h_S$\"\n",
    "\n",
    "# Plot true dividing lines\n",
    "x1 = XX1[:, 0]\n",
    "plt.plot(x1, (x1**2 - 3*x1 - 2*0)/3, '-k', label = \"truth\")\n",
    "plt.plot(x1, (x1**2 - 3*x1 - 2*np.pi)/3, '-k')\n",
    "plt.plot(x1, (x1**2 - 3*x1 + 2*np.pi)/3, '-k')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'$ \\kappa $ = {kappa:.3f}', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e0d25",
   "metadata": {},
   "source": [
    "#### (2.2) Plot RKHS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(7, 7))\n",
    "\n",
    "ax.plot_surface(XX1, XX2, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid point matrice\n",
    "M = np.c_[XX1.ravel(), XX2.ravel()]\n",
    "print(M.shape)\n",
    "print(len(M))\n",
    "print(M[1][:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd74def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of support vectors\n",
    "SV_supp = support_vectors.T\n",
    "\n",
    "# Number of support vectors\n",
    "m_supp = SV_supp.shape[1]\n",
    "print(m_supp)\n",
    "\n",
    "# print(SV_supp)\n",
    "# print(np.tile(M[1][:, None], (1, m_supp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned coefficients\n",
    "alpha_S = KSVM.dual_coef_.T\n",
    "print(alpha_S.shape)\n",
    "\n",
    "b_S = KSVM.intercept_\n",
    "print(b_S)\n",
    "\n",
    "kappa_S = kappa\n",
    "print(kappa_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance function to the support vectors\n",
    "def SV_fun(x): return np.sum((SV_supp - np.tile(x, (1, m_supp)))**2, axis=0)[:, None].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b133eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned RKHS function\n",
    "def kfun_S(x): return np.dot(np.exp(-kappa_S * SV_fun(x)), alpha_S) + b_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of the RKHS function on the grid\n",
    "Z = np.empty((len(M), 1))\n",
    "for i in range(len(M)):\n",
    "    Z[i] = kfun_S(M[i][:, None])\n",
    "print(Z)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(7, 7))\n",
    "\n",
    "ax.plot_surface(XX1, XX2, np.reshape(Z, (len(x1), len(x1))), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7731501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D-Plot\n",
    "\n",
    "C = np.max(Z)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(10, 10))\n",
    "\n",
    "ax.plot_surface(XX1, XX2, np.reshape(Z-C, (len(x1), len(x1))), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "# Plot learned dividing lines\n",
    "plt.contour(XX1, XX2, np.reshape(Z, (len(x1), len(x1))), levels = 0)  # label = \"$h_S$\"\n",
    "\n",
    "ax.scatter(x[0][indp], x[1][indp], c=\"b\", marker=\"o\", linewidths = 2, label = \"1\")\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"+\", linewidths = 2, label = \"-1\")\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(support_vectors[:, 0], support_vectors[:, 1], marker=\"o\", markersize=12, \n",
    "         markerfacecolor=\"None\", linestyle='None', label = \"Support Vectors\")\n",
    "\n",
    "ax.view_init(90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2911d57",
   "metadata": {},
   "source": [
    "#### (3) Kappa study (Gaussian kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling parameter\n",
    "kappa = 1/(0.075)**2\n",
    "# kappa = 1/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a support vector classifier\n",
    "KSVM_kappa = svm.SVC(C=0.5/m/lam, kernel='rbf', gamma=kappa)\n",
    "\n",
    "# Learn on the train subset\n",
    "KSVM_kappa.fit(x.T, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get support vectors\n",
    "support_vectors_kappa = KSVM_kappa.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7facdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision function\n",
    "Z = KSVM_kappa.decision_function(np.c_[XX1.ravel(), XX2.ravel()])\n",
    "print('Z', Z)\n",
    "\n",
    "Z = Z.reshape(XX1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "indp = [i for (i, val) in enumerate(y) if val == 1]\n",
    "indm = [i for (i, val) in enumerate(y) if val == -1]\n",
    "\n",
    "ax.scatter(x[0][indp], x[1][indp], c=\"b\", marker=\"o\", linewidths = 2, label = \"1\")\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"+\", linewidths = 2, label = \"-1\")\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(support_vectors_kappa[:, 0], support_vectors_kappa[:, 1], marker=\"o\", markersize=12, \n",
    "         markerfacecolor=\"None\", linestyle='None', label = \"Support Vectors\")\n",
    "\n",
    "# Plot learned dividing lines\n",
    "plt.contour(XX1, XX2, Z, levels = 0)  # label = \"$h_S$\"\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "# plt.legend()\n",
    "plt.title(f'$ \\kappa $ = {kappa:.3f}', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6d1a4",
   "metadata": {},
   "source": [
    "#### (4) Lambda study (Gaussian kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda for Soft Rule\n",
    "# lam = 0.5/m;\n",
    "# lam = 1/(2*m)^2;\n",
    "lam = 0.035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94661974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling parameter\n",
    "kappa = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a support vector classifier\n",
    "KSVM_lam = svm.SVC(C=0.5/m/lam, kernel='rbf', gamma=kappa)\n",
    "\n",
    "# Learn on the train subset\n",
    "KSVM_lam.fit(x.T, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d05767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get support vectors\n",
    "support_vectors_lam = KSVM_lam.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision function\n",
    "Z = KSVM_lam.decision_function(np.c_[XX1.ravel(), XX2.ravel()])\n",
    "print('Z', Z)\n",
    "\n",
    "Z = Z.reshape(XX1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9469aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "indp = [i for (i, val) in enumerate(y) if val == 1]\n",
    "indm = [i for (i, val) in enumerate(y) if val == -1]\n",
    "\n",
    "ax.scatter(x[0][indp], x[1][indp], c=\"b\", marker=\"o\", linewidths = 2, label = \"1\")\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"+\", linewidths = 2, label = \"-1\")\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(support_vectors_lam[:, 0], support_vectors_lam[:, 1], marker=\"o\", markersize=12, \n",
    "         markerfacecolor=\"None\", linestyle='None', label = \"Support Vectors\")\n",
    "\n",
    "# Plot learned dividing lines\n",
    "plt.contour(XX1, XX2, Z, levels = 0)  # label = \"$h_S$\"\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "# plt.legend()\n",
    "plt.title(f'$ \\lambda $ = {lam:.3f}', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8d184",
   "metadata": {},
   "source": [
    "#### (5) Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5449e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lam = 0.5/m\n",
    "kappa = 1\n",
    "q = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c47c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a support vector classifier\n",
    "KSVM_poly = svm.SVC(C=0.5/m/lam, kernel='poly', gamma=kappa, degree=3, coef0=1)\n",
    "\n",
    "# Learn on the train subset\n",
    "KSVM_poly.fit(x.T, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41d5cb",
   "metadata": {},
   "source": [
    "###### IMPORTANT\n",
    "\n",
    "Difference between the formulas in Matlab and Sklearn\n",
    "See\n",
    "https://scikit-learn.org/stable/modules/svm.html 1.4.6 Kernel functions\n",
    "https://de.mathworks.com/help/stats/fitcsvm.html#bt9w6j6_sep_shared-PolynomialOrder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9efad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get support vectors\n",
    "support_vectors_poly = KSVM_poly.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision function\n",
    "Z = KSVM_poly.decision_function(np.c_[XX1.ravel(), XX2.ravel()])\n",
    "print('Z', Z)\n",
    "\n",
    "Z = Z.reshape(XX1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5515d",
   "metadata": {},
   "source": [
    "#### (5.1) Plot the dividing lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "indp = [i for (i, val) in enumerate(y) if val == 1]\n",
    "indm = [i for (i, val) in enumerate(y) if val == -1]\n",
    "\n",
    "ax.scatter(x[0][indp], x[1][indp], c=\"b\", marker=\"o\", linewidths = 2, label = \"1\")\n",
    "ax.scatter(x[0][indm], x[1][indm], c=\"r\", marker=\"+\", linewidths = 2, label = \"-1\")\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(support_vectors_poly[:, 0], support_vectors_poly[:, 1], marker=\"o\", markersize=12, \n",
    "         markerfacecolor=\"None\", linestyle='None', label = \"Support Vectors\")\n",
    "\n",
    "# Plot learned dividing lines\n",
    "plt.contour(XX1, XX2, Z, levels = 0)  # label = \"$h_S$\"\n",
    "\n",
    "# Plot true dividing lines\n",
    "x1 = XX1[:, 0]\n",
    "plt.plot(x1, (x1**2 - 3*x1 - 2*0)/3, '-k', label = \"truth\")\n",
    "plt.plot(x1, (x1**2 - 3*x1 - 2*np.pi)/3, '-k')\n",
    "plt.plot(x1, (x1**2 - 3*x1 + 2*np.pi)/3, '-k')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Polynomial Kernel (q={q})', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ff0dd",
   "metadata": {},
   "source": [
    "#### (5.2) Plot RKHS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f42343",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(7, 7))\n",
    "\n",
    "ax.plot_surface(XX1, XX2, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid point matrice\n",
    "M = np.c_[XX1.ravel(), XX2.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of support vectors\n",
    "SV_supp = support_vectors_poly.T\n",
    "\n",
    "# Number of support vectors\n",
    "m_supp = SV_supp.shape[1]\n",
    "print(m_supp)\n",
    "\n",
    "# print(SV_supp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned coefficients\n",
    "alpha_S = KSVM_poly.dual_coef_.T\n",
    "print(alpha_S.shape)\n",
    "\n",
    "b_S = KSVM_poly.intercept_\n",
    "print(b_S)\n",
    "\n",
    "kappa_S = kappa\n",
    "print(kappa_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance function to the support vectors\n",
    "def SV_fun(x): return np.sum((SV_supp * np.tile(x, (1, m_supp))), axis=0)[:, None].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0901c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned RKHS function\n",
    "def kfun_S(x): return np.dot((1 + kappa_S * SV_fun(x))**q, alpha_S) + b_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7aea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of the RKHS function on the grid\n",
    "Z = np.empty((len(M), 1))\n",
    "for i in range(len(M)):\n",
    "    Z[i] = kfun_S(M[i][:, None])\n",
    "print(Z)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e21739",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(7, 7))\n",
    "\n",
    "ax.plot_surface(XX1, XX2, np.reshape(Z, (len(x1), len(x1))), cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c192da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
